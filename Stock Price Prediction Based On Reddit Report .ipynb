{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built a tool to predict stock prices from discussion on Reddit's [/r/WallStreetBets](reddit.com/r/WallStreetBets) forum by scraping the previous 100 day's posts and comments from the subreddit, and collecting historical data about stock prices from the [Marketstack API](https://marketstack.com/). Our predictor works by identifying days with similar amounts of change (after computing [z-score](https://www.statisticshowto.com/probability-and-statistics/z-score/)) in the amount of mentions of a stock's symbol. To validate our method, we compare the predicted change in the price (rise, fall, or no change) with the actual change in price. Overall, our model struggles to achieve accurate results. We suggest that it does so because of the difficulty of predicting something with as many factors as the movement of a stock's price based solely off the change in the number of times it is mentioned on one forum.\n",
    "\n",
    "# Data Description\n",
    "\n",
    "## Reddit posts and comments\n",
    "After scraping and cleaning the posts and comments, we have DataFrames containing posts and comments on WallStreetBets from the last 100 days. In order to try to cut down on the amount of spam and joke posts, we filtered posts by the user-assigned flairs that mark them either as `Discussion`, where users discuss things that are happening in the stock market, or `DD`, where the author of the post shares their analysis of a particular stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_posts = pd.read_csv('posts_last_100_days.csv', index_col = 'Unnamed: 0')\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = pd.read_csv('comments_last_100_days.csv', index_col = 'Unnamed: 0')\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then computed the number of times each stock was mentioned per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_symbol_counts = pd.read_csv('symbol_counts.csv', index_col=[0,1])\n",
    "df_symbol_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to see how many times each stock symbol was mentioned on each day that we have data for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_tsla = df_symbol_counts.unstack(level=1).loc[['TSLA']]\n",
    "\n",
    "df_counts_tsla.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model, we converted the raw number of mentions to the zscore, in order to have a standardized metric for how much the discussion on each day varies. When we apply this, the DataFrame above looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mentions to zscores\n",
    "# transpose df_counts_final to make zscore operation easieer, will transpose back later\n",
    "df_counts_tsla_t = df_counts_tsla.T\n",
    "\n",
    "# find zscore of mentions of each day\n",
    "from scipy.stats import zscore\n",
    "df_counts_tsla_t = df_counts_tsla_t.apply(zscore)\n",
    "\n",
    "# undo transpose\n",
    "df_counts_tsla = df_counts_tsla_t.T\n",
    "df_counts_tsla.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbols with the highest variance in discussion\n",
    "After finding how many times each stock was mentioned each day, we found the stocks with the 10 highest amounts of change in the number of mentions. We used standard deviation for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_unstack = df_symbol_counts.unstack(level=1)\n",
    "df_symbol_counts_t = df_count_unstack.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out which symbols have the most variance in mentions\n",
    "std_series = df_symbol_counts_t.std()\n",
    "\n",
    "# get top 10 symbols with highest standard deviations\n",
    "high_std_symbol = std_series.sort_values(ascending=False)[:10]\n",
    "\n",
    "# list of symbols with highest variance\n",
    "symbol_list = high_std_symbol.index.tolist()\n",
    "\n",
    "print(symbol_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marketstack Data\n",
    "\n",
    "To collect data from Marketstack's API, we wrote a function that would return a specified stock's price history as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def ticker_history(ticker):\n",
    "    \"\"\" returns trading data of a stock over the last 100 trading days. \n",
    "    Args:\n",
    "        ticker (string): ticker symbol representing inputed stock\n",
    "        \n",
    "    Returns:\n",
    "        df_history (DataFrame): pd DF containing end of day data \n",
    "    \"\"\"\n",
    "    # define api acess key\n",
    "    params = {'access_key': 'fb8442e7289f98db111f57de2a4c1d75'}\n",
    "    \n",
    "    # call to api with inputed parameters\n",
    "    api_result = requests.get(f'https://api.marketstack.com/v1/tickers/{ticker}/eod', params)\n",
    "    \n",
    "    # convert data to json format\n",
    "    api_response = api_result.json()\n",
    "    \n",
    "    # DataFrame composed of only end of day stock data\n",
    "    df_history = pd.DataFrame(api_response['data']['eod'])\n",
    "    \n",
    "    return df_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsla_history = ticker_history('tsla')\n",
    "df_tsla_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method\n",
    "\n",
    "## 5-NN regressor\n",
    "To predict price movements, we use an 5-NN regressor. In essence, we estimate the predicted movement by looking at the 5 days with the most similar zscores. We chose 5 as the number of neighbors after experimenting with different numbers of neighbors to find a balance between result accuracy and skewed predictions (i.e the model predicting everything as one category)\n",
    "\n",
    "We decided to use the K-Nearest Neighbors algorithm because it is the closest to our original idea, of finding a correlation between days with similar amounts of change in discussion and movements in stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation 1 - SNDL\n",
    "We chose to train our first model on SNDL's stock because that was the stock with the highest variance in the number of daily mentions, so we thought that it had the best potential to show a relationship between price movements and changes in discussion. However, it also had many days where it was not mentioned at all, as can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_sndl = df_count_unstack.loc[['SNDL']]\n",
    "\n",
    "df_counts_sndl.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, we computed the z-score of each day's number of mentions in order to have a standardized metric for how much each day's level of discussion varies from the mean, rather than just using the raw number of mentions. After computing the z-score for each day and joining the number of mentions with the percent change in SNDL's closing price, we had the below DataFrame, which we used to train our K-Nearest Neighbors classsifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mentions to z-scores\n",
    "# transpose df_counts_final to make zscore operation easieer, will transpose back later\n",
    "df_counts_sndl_t = df_counts_sndl.T\n",
    "\n",
    "# find zscore of mentions of each day\n",
    "from scipy.stats import zscore\n",
    "df_counts_sndl_t = df_counts_sndl_t.apply(zscore)\n",
    "\n",
    "# undo transpose\n",
    "df_counts_sndl = df_counts_sndl_t.T\n",
    "df_counts_sndl.T.head()\n",
    "# transpose df_counts_sndl so date is recorded in columns, like df_sndl_history\n",
    "df_counts_sndl.T\n",
    "# remove heirarchical index\n",
    "df_sndl_no_multi_index = df_counts_sndl.T.reset_index()\n",
    "df_sndl = df_sndl_no_multi_index.reset_index()\n",
    "\n",
    "# remove unneccesary columns\n",
    "del df_sndl['level_0']\n",
    "del df_sndl['index']\n",
    "\n",
    "df_sndl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Price movements -- if the close price rose or fell each day\n",
    "# get price history\n",
    "df_sndl_history = ticker_history('sndl')\n",
    "\n",
    "# make sure that it is sorted by date so that percent change in price is a meaningful measure\n",
    "df_sndl_history['date'] = pd.to_datetime(df_sndl_history['date']).dt.date\n",
    "df_sndl_history = df_sndl_history.sort_values(by='date')\n",
    "\n",
    "# create column for percent change as a percentage\n",
    "df_sndl_history['close % change'] = df_sndl_history['close'].pct_change()\n",
    "\n",
    "# create categories from percent change -- rise, fall, no change\n",
    "df_sndl_history.loc[df_sndl_history['close % change'] > 0, 'rise/fall'] = 'rise'\n",
    "df_sndl_history.loc[df_sndl_history['close % change'] < 0, 'rise/fall'] = 'fall'\n",
    "df_sndl_history.loc[df_sndl_history['close % change'] == 0, 'rise/fall'] = 'no change'\n",
    "\n",
    "df_sndl_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine datasets (reddit + marketstack)\n",
    "# ensure both date columns are datetime.date objects for consistency\n",
    "df_sndl['date'] = pd.to_datetime(df_sndl['date']).dt.date\n",
    "df_sndl_history['date'] = pd.to_datetime(df_sndl_history['date']).dt.date\n",
    "\n",
    "\n",
    "# remove dates that aren't in both dataframes -- account for markets being closed on weekends and other factors\n",
    "date_list = df_sndl['date'].tolist()\n",
    "\n",
    "df_sndl_history = df_sndl_history[df_sndl_history['date'].isin(date_list)]\n",
    "\n",
    "sndl_date_list = df_sndl_history['date'].tolist()\n",
    "df_sndl = df_sndl[df_sndl['date'].isin(sndl_date_list)]\n",
    "\n",
    "df_sndl_history = df_sndl_history.sort_values(by='date')\n",
    "df_sndl = df_sndl.sort_values(by='date')\n",
    "\n",
    "# add price movements column - we can just assign a copy since we know both dataframes \n",
    "# only contain the same dates and are sorted in the same way\n",
    "df_sndl['price movement'] = df_sndl_history['rise/fall'].values\n",
    "\n",
    "df_sndl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# split columns into x (features) and y (labels)\n",
    "x_feat_list = ['SNDL']\n",
    "y_feat = 'price movement'\n",
    "\n",
    "\n",
    "# get x and y for classifiers\n",
    "x = df_sndl.loc[:, x_feat_list].values\n",
    "y_true = df_sndl.loc[:, y_feat].values\n",
    "\n",
    "# we will be using 10 fold cross validation\n",
    "n_splits = 10\n",
    "\n",
    "# dictionary for confusion matrix\n",
    "conf_matrix_sndl_dict = {}\n",
    "\n",
    "# number of neighbors to use\n",
    "k = 5\n",
    "\n",
    "# initialize knn_classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# cross validation\n",
    "kfold = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "# y_pred is empty array same size as y_true\n",
    "y_pred = np.empty_like(y_true)\n",
    "\n",
    "# for plotting\n",
    "x_test_total = np.empty_like(x)\n",
    "\n",
    "for train_idx, test_idx in kfold.split(x, y_true):\n",
    "    # training data\n",
    "    x_train = x[train_idx, :]\n",
    "    y_true_train = y_true[train_idx]\n",
    "\n",
    "    # testing data\n",
    "    x_test = x[test_idx, :]\n",
    "    y_true_test = y_true[test_idx]\n",
    "    \n",
    "    x_test_total[test_idx] = x_test\n",
    "\n",
    "    # train on training data\n",
    "    knn_classifier.fit(x_train, y_true_train)\n",
    "\n",
    "    # estimat each song's genre\n",
    "    y_pred[test_idx] = knn_classifier.predict(x_test)\n",
    "\n",
    "\n",
    "# generate confusion matrix\n",
    "conf_matrix_sndl = confusion_matrix(y_true=y_true, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix for k = 5\n",
    "conf_matrix_sndl_plot = ConfusionMatrixDisplay(conf_matrix_sndl, display_labels=np.unique(y_true))\n",
    "conf_matrix_sndl_plot.plot()\n",
    "plt.suptitle('K=5 NN Classifier for SNDL\\'s price movement');\n",
    "# save figure\n",
    "plt.savefig('sndl_k5_conf_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12,10))\n",
    "# order for violins\n",
    "order = ['rise', 'no change', 'fall']\n",
    "\n",
    "# variables for plotting actual values\n",
    "price_movement_actual_sndl = np.array(df_sndl['price movement'].values)\n",
    "z_score_actual_sndl = np.array(df_sndl['SNDL'].values, dtype=float)\n",
    "\n",
    "# plot actual values\n",
    "sns.violinplot(x=price_movement_actual_sndl, \n",
    "               y=z_score_actual_sndl, \n",
    "               ax=ax[0], \n",
    "               order=order)\n",
    "\n",
    "# variables for plotting predicted values\n",
    "price_movement_pred_sndl = y_pred\n",
    "z_score_pred_sndl = x_test_total.reshape(52,)\n",
    "sns.violinplot(x=price_movement_pred_sndl, \n",
    "               y=z_score_pred_sndl, \n",
    "               ax=ax[1], \n",
    "               order=order)\n",
    "\n",
    "# labelling\n",
    "ax[0].set_title(\"Actual distribution of SNDL daily mention z-scores per category\")\n",
    "plt.ylabel('z-score')\n",
    "ax[1].set_title(\"Predicted distribution of SNDL daily mention z-scores per category\")\n",
    "\n",
    "# formatting and saving\n",
    "fig.tight_layout()\n",
    "fig.savefig('sndl_violinplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation 2 - TSLA\n",
    "After seeing how our initial model's poor performance when it tried to predict SNDL's price movements, we decided to try training a new model, this time on TSLA's price movements and daily mentions. We chose to use TSLA as it had a similarly high variance to SNDL, but it had more daily mentions overall, as can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_tsla = df_count_unstack.loc[['TSLA']]\n",
    "\n",
    "df_counts_tsla.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mentions to z-scores\n",
    "# transpose df_counts_final to make zscore operation easieer, will transpose back later\n",
    "df_counts_tsla_t = df_counts_tsla.T\n",
    "\n",
    "# find zscore of mentions of each day\n",
    "from scipy.stats import zscore\n",
    "df_counts_tsla_t = df_counts_tsla_t.apply(zscore)\n",
    "\n",
    "# undo transpose\n",
    "df_counts_tsla = df_counts_tsla_t.T\n",
    "\n",
    "# transpose df_counts_sndl so date is recorded in columns, like df_sndl_history\n",
    "df_counts_tsla.T\n",
    "# remove heirarchical index\n",
    "df_tsla_no_multi_index = df_counts_tsla.T.reset_index()\n",
    "df_tsla = df_tsla_no_multi_index.reset_index()\n",
    "\n",
    "# remove unneccesary columns\n",
    "del df_tsla['level_0']\n",
    "del df_tsla['index']\n",
    "\n",
    "df_tsla.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Price movements -- if the close price rose or fell each day\n",
    "# get price history\n",
    "df_tsla_history = ticker_history('tsla')\n",
    "\n",
    "# make sure that it is sorted by date so that percent change in price is a meaningful measure\n",
    "df_tsla_history['date'] = pd.to_datetime(df_tsla_history['date']).dt.date\n",
    "df_tsla_history = df_tsla_history.sort_values(by='date')\n",
    "\n",
    "# create column for percent change as a percentage\n",
    "df_tsla_history['close % change'] = df_tsla_history['close'].pct_change()\n",
    "\n",
    "# create categories from percent change -- rise, fall, no change\n",
    "df_tsla_history.loc[df_tsla_history['close % change'] > 0, 'rise/fall'] = 'rise'\n",
    "df_tsla_history.loc[df_tsla_history['close % change'] < 0, 'rise/fall'] = 'fall'\n",
    "df_tsla_history.loc[df_tsla_history['close % change'] == 0, 'rise/fall'] = 'no change'\n",
    "\n",
    "df_tsla_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine datasets (reddit + marketstack)\n",
    "# ensure both date columns are datetime.date objects for consistency\n",
    "df_tsla['date'] = pd.to_datetime(df_tsla['date']).dt.date\n",
    "df_tsla_history['date'] = pd.to_datetime(df_tsla_history['date']).dt.date\n",
    "\n",
    "\n",
    "# remove dates that aren't in both dataframes -- account for markets being closed on weekends and other factors\n",
    "date_list = df_tsla['date'].tolist()\n",
    "\n",
    "df_tsla_history = df_tsla_history[df_tsla_history['date'].isin(date_list)]\n",
    "\n",
    "tsla_date_list = df_tsla_history['date'].tolist()\n",
    "df_tsla = df_tsla[df_tsla['date'].isin(tsla_date_list)]\n",
    "\n",
    "df_tsla_history = df_tsla_history.sort_values(by='date')\n",
    "df_tsla = df_tsla.sort_values(by='date')\n",
    "\n",
    "# add price movements column - we can just assign a copy since we know both dataframes \n",
    "# only contain the same dates and are sorted in the same way\n",
    "df_tsla['price movement'] = df_tsla_history['rise/fall'].values\n",
    "\n",
    "df_tsla.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split columns into x (features) and y (labels)\n",
    "x_feat_list = ['TSLA']\n",
    "y_feat = 'price movement'\n",
    "\n",
    "\n",
    "# get x and y for classifiers\n",
    "x = df_tsla.loc[:, x_feat_list].values\n",
    "y_true = df_tsla.loc[:, y_feat].values\n",
    "\n",
    "# we will be using 10 fold cross validation\n",
    "n_splits = 10\n",
    "\n",
    "# dictionary for confusion matrix\n",
    "conf_matrix_tsla_dict = {}\n",
    "\n",
    "# number of neighbors to use\n",
    "k = 5\n",
    "\n",
    "# initialize knn_classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# cross validation\n",
    "kfold = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "# y_pred is empty array same size as y_true\n",
    "y_pred = np.empty_like(y_true)\n",
    "\n",
    "# for plotting\n",
    "x_test_total = np.empty_like(x)\n",
    "\n",
    "for train_idx, test_idx in kfold.split(x, y_true):\n",
    "    # training data\n",
    "    x_train = x[train_idx, :]\n",
    "    y_true_train = y_true[train_idx]\n",
    "\n",
    "    # testing data\n",
    "    x_test = x[test_idx, :]\n",
    "    y_true_test = y_true[test_idx]\n",
    "    \n",
    "    x_test_total[test_idx] = x_test\n",
    "\n",
    "    # train on training data\n",
    "    knn_classifier.fit(x_train, y_true_train)\n",
    "\n",
    "    # estimat each song's genre\n",
    "    y_pred[test_idx] = knn_classifier.predict(x_test)\n",
    "\n",
    "\n",
    "# generate confusion matrix\n",
    "conf_matrix_tsla = confusion_matrix(y_true=y_true, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix for k = 5\n",
    "conf_matrix_tsla_plot = ConfusionMatrixDisplay(conf_matrix_tsla, display_labels=np.unique(y_true))\n",
    "conf_matrix_tsla_plot.plot()\n",
    "plt.suptitle('K=5 NN Classifier for TSLA\\'s price movement');\n",
    "# save figure\n",
    "plt.savefig('tsla_k5_conf_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,10))\n",
    "\n",
    "# order for violinplots\n",
    "order = ['rise', 'no change', 'fall']\n",
    "\n",
    "# true values for plotting\n",
    "price_movement_actual = np.array(df_tsla['price movement'].values)\n",
    "z_score_actual = np.array(df_tsla['TSLA'].values, dtype=float)\n",
    "\n",
    "# plot actual values\n",
    "sns.violinplot(x=price_movement_actual, \n",
    "               y=z_score_actual, \n",
    "               ax=ax[0], \n",
    "               order=order)\n",
    "\n",
    "# predicted values for plotting\n",
    "price_movement_pred = y_pred\n",
    "z_score_pred = x_test_total.reshape(52,)\n",
    "\n",
    "# plot predicted valies\n",
    "sns.violinplot(x=price_movement_pred, \n",
    "               y=z_score_pred, \n",
    "               ax=ax[1],\n",
    "               order=order)\n",
    "\n",
    "# labeling\n",
    "ax[0].set_title(\"Actual distribution of TSLA daily mention z-scores per category\")\n",
    "plt.ylabel('z-score')\n",
    "ax[1].set_title(\"Predicted distribution of TSLA daily mention z-scores per category\")\n",
    "\n",
    "# formatting and saving\n",
    "fig.tight_layout()\n",
    "fig.savefig('tsla_violinplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "As can be seen above, neither of our models were able to consistently make good predictions about the movement of \\\\$SNDL or \\\\$TSLA's price solely based on the change in amount of discussion on /r/WallStreetBets. By the time we got to the point in our project where we were training our models and validating their results, we had expected an outcome similar to this for several reasons, namely:\n",
    "- Stock prices can be highly volatile, and predicting their movements solely from the change in discussion on one subreddit did not give us enough information to be able to make an accurate prediction \n",
    "- We only considered the number of times each stock symbol was mentioned, without the context of whether it was being discussed positively or negatively, so on days where the price drops considerably, there could have been more negative discussion, while on days where the price rose considerable, there could have been more positive discussion, but our model has no way of being aware of that distinction\n",
    "- /r/WallStreetBets as a whole is more focused on memes and jokes than serious discussion of the stock market, and while there definitely are instances of high-quality analysis of the stock market, and people who invest intelligently, filtering out the memes and the spam posts proved to be more of a challenge than we were expecting.\n",
    "\n",
    "Additionally, early on in our data collection, we placed a focus on collecting all available information from the subreddit, meaning as many posts as we could reasonably scrape, and all the comments of those posts, which meant that, due to the amount of time taken to scrape and clean this data, we did not have data going back more than a few months, so it is possible that if we had focused on scraping fewer posts per day, but from a larger time period, we may have achieved better results, as we would have had a more robust dataset.\n",
    "\n",
    "We also made the decision to only look at stocks listed on the NASDAQ, again due to the time taken to count how many times each symbol was mentioned, so it is possible that we would have been able to achieve better results for a popular stock like \\\\$GME, that isn't listed on the NASDAQ.\n",
    "\n",
    "To remedy these shortcomings in a future version of this project, we would like to source information from a wider range of investing-related subreddits that are more serious that /r/WallStreetBets to see if more serious discussion would yield better results. \n",
    "\n",
    "We would also like to account for the fact that discussion of a stock has the potential to increase both when the price rises and when the price falls by including information about the sentiment of the day's comments, for example if they are overwhelmingly positive, overwhelmingly negative, or somewhere in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "We do not believe that anyone should be picking which stocks to buy or sell based on the results of our project due to the low accuracies of both our models, however, it could provide a good basis for someone looking to scrape online discussion for another project, whether it was about the stock market or not. Before doing so, however, they should carefully consider the source of the information that they are scraping, specifically whether the people who post on the site or forum tend to make serious remarks or jokes, as this could potentially have an effect on the quality of their final results.\n",
    "\n",
    "As far as our own work is concerned, we do not believe that there are any major ethical concerns with either the data we collected, as it is all posted publically under the users' usernames, rather than their real names, and we do not consider the actual content of either the posts or comments beyond how many times they mention a particular stock's ticker. Similarly, we do not believe that there are ethical concerns with our results, as we are recomending that they not be used as the basis for buying and selling stocks due to the low accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
